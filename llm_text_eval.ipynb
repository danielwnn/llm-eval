{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDnxp14en-V8",
        "outputId": "32046012-be95-4099-b934-6d1f479ee3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: rouge_score in /root/.local/lib/python3.10/site-packages (0.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# install the latest VertexAI and langchain package\n",
        "! pip install nltk rouge_score --upgrade --user"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLEU (Bilingual Evaluation Understudy)**\n",
        "\n",
        "BLEU measures the n-gram overlap between the generated text and a reference text. It is a widely used metric, but it has been criticized for being insensitive to fluency and coherence.\n"
      ],
      "metadata": {
        "id": "pQa0XeJGpISt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Reference text and generated text\n",
        "reference = [\"The quick brown fox jumps over the lazy dog.\"]\n",
        "generated = \"A quick brown fox jumps over a lazy dog.\"\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu(reference, generated)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlITI4tboUTm",
        "outputId": "c0213444-0c69-49cd-b28b-699f3b33aee2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.8212432636664931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: ROUGE measures the overlap between the generated text and a reference text, but it is more lenient than BLEU and is better at capturing fluency and coherence.\n",
        "- **ROUGE-N**: Overlap of n-grams between the system and reference summaries.\n",
        "  - **ROUGE-1** refers to overlap of unigrams between the generated text and reference text.\n",
        "  - **ROUGE-2** refers to the overlap of bigrams between the generated text and reference text.\n",
        "- **ROUGE-L**: Longest Common Subsequence (LCS) based statistics. Longest common subsequence problem takes into account sentence-level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.\n",
        "- **ROUGE-W**: Weighted LCS-based statistics that favors consecutive LCSes.\n",
        "- **ROUGE-S**: Skip-bigram based co-occurrence statistics. Skip-bigram is any pair of words in their sentence order.\n",
        "- **ROUGE-SU**: Skip-bigram plus unigram-based co-occurrence statistics.\n",
        "\n"
      ],
      "metadata": {
        "id": "R7sOES7YVTT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROUGE vs BLEU**\n",
        "\n",
        "- **BLEU** focuses on precision: how much the words (and/or n-grams) in the candidate model outputs appear in the human reference.\n",
        "- **ROUGE** focuses on recall: how much the words (and/or n-grams) in the human references appear in the candidate model outputs.\n",
        "\n",
        "These results are complementing, as is often the case in the precision-recall tradeoff."
      ],
      "metadata": {
        "id": "A-9K1Gf_aUHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(\"The quick brown fox jumps over the lazy dog.\", \"A brown fox jumps over a lazy dog.\")\n",
        "\n",
        "print(\"ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {scores['rouge1']}\")\n",
        "print(f\"ROUGE-2: {scores['rouge2']}\")\n",
        "print(f\"ROUGE-L: {scores['rougeL']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPJr-w6QSfI7",
        "outputId": "0462906d-0244-45ed-90ec-57b29cd013dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores:\n",
            "ROUGE-1: Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765)\n",
            "ROUGE-2: Score(precision=0.5714285714285714, recall=0.5, fmeasure=0.5333333333333333)\n",
            "ROUGE-L: Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libs:\n",
        "- https://www.nltk.org/\n",
        "- https://keras.io/api/keras_nlp/metrics/\n",
        "- https://pypi.org/project/rouge-score/"
      ],
      "metadata": {
        "id": "5YBPFW4Lbsqw"
      }
    }
  ]
}